\documentclass[11pt]{article}
\usepackage{geometry}
\linespread{1.02}
\usepackage{url,epsfig}
\usepackage{relsize}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[nosectionbib]{apacite} %apa citation style
\bibliographystyle{apacite}
\graphicspath{{./fig/}}

% For R code snippets
\usepackage{listings}

% Bibliography spacing/indentation
%\makeatother\renewcommand{\bibitem}{\vskip 2pt\par\hangindent\parindent\hskip-\parindent}

% Define a bunch of custom math shortcuts for expressions used many times
% throughout the paper
\newcommand{\Rsq}{$r^2\,$}
\newcommand{\boldrho}{\boldsymbol{\rho}}
\newcommand{\boldbeta}{\boldsymbol{\beta}}
\newcommand{\boldtheta}{\boldsymbol{\theta}}
\newcommand{\hatbeta}{\widehat{\boldbeta}}
\newcommand{\hatalpha}{\widehat{\alpha}}
\newcommand{\sigmaEps}{\sigma_{\epsilon}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\Q}{\mathbf{Q}}
\newcommand{\R}{\mathbf{R}}
\renewcommand{\u}{\mathbf{u}}
\newcommand{\locRsq}{\ell_{r^2}}
\newcommand{\halfK}{\frac{K}{2}}
\newcommand{\Betadist}[2]{\mathrm{Beta}\left(#1,#2\right)}
\newcommand{\Digamma}[1]{\psi\left(#1\right)}
\newcommand{\given}{\left.\right|}


% Title, authors, date
\title{\bf Regularizing Bayesian linear models with an informative prior on \Rsq
    \vspace{.1in}}
\author{Ben Goodrich\footnote{Columbia University}
    \and Jonah Gabry$^{\ast}$
    \and Andrew Gelman$^{\ast}$
    \vspace{.1in}}
\date{29 February 2016
    \vspace{-.2in}}

% Begin document
\begin{document}
\maketitle
\thispagestyle{empty}

\begin{abstract}
\noindent We derive an approach for expressing prior beliefs about the location
of the \Rsq, the familiar proportion of variance in the outcome variable that is
attributable to the predictors under a linear model. In particular, when there
are many predictors relative to the number of observations we would expect the
joint prior derived here to work better than placing independent, heavy-tailed
priors on the coefficients, which is  standard practice in applied Bayesian data
analysis but neither reflects the beliefs of the researcher nor conveys enough
information to stabilize all the computations.
\end{abstract}


\section{Introduction}

Fully making Bayesian estimation of linear models routine for applied
researchers requires prior distributions that work well for any data generated
according to the assumptions of the likelihood function. Most Bayesian
approaches require the researcher to specify a joint prior distribution for the
regression coefficients (and the intercept and error variance), but most applied
researchers have little inclination to specify all of these prior distributions
thoughtfully and take a shortcut by specifying a single prior distribution that
is taken to apply to all regression coefficients as if they were independent of
each other (and the intercept and error variance).

In this paper we derive and demonstrate an approach for expressing  prior
beliefs about the location of the \Rsq, the familiar proportion of variance in
the outcome variable that is attributable to the predictors under a linear
model. Since the \Rsq is a well-understood bounded scalar, it is easy to specify
prior information about it. In particular, when there are many predictors
relative to the number of observations we would expect the joint prior derived
here to work better than placing independent, heavy-tailed priors on the
coefficients, which neither reflects the beliefs of the researcher nor conveys
enough information to stabilize all the computations.


\section{QR-reparameterized likelihood}
\label{sec:likelihood}

The likelihood contribution for one observation $y_i$ under a linear model
can be written as the conditionally normal density

$$
f \left(y_i \given \mu_i, \sigmaEps \right) = \frac{1}{\sigmaEps \sqrt{2 \pi}}
\exp{\left\{-\frac{1}{2} \left(\frac{y_i - \mu_i}{\sigmaEps}\right)^2\right\}},
$$
%
where $\mu_i = \alpha + \mathbf{x}_i^\top \boldbeta$ is a linear
predictor and $\sigmaEps$ is the standard deviation of the error in predicting
the outcome. For a sample of size $N$, the likelihood of the entire sample is
the product of the $N$ individual likelihood contributions and it is well known
that the likelihood is maximized when the sum-of-squared residuals is minimized.
This occurs when
%
\begin{align*}
\hatbeta &= \left(\X^\top \X \right)^{-1} \X^\top \y,\\
\hatalpha &= \overline{y} - \overline{\mathbf{x}}^\top \hatbeta,\\
\widehat{\sigma}_{\epsilon}^2 &=
  \frac{1}{N}
  \left(\y - \hatalpha - \X \hatbeta \right)^\top
  \left(\y - \hatalpha - \X \hatbeta \right),
\end{align*}
%
where $\overline{\mathbf{x}}$ is a vector of sample means for the
$K$ predictors, $\X$ is a $N \times K$ matrix of \emph{centered} predictors,
$\y$ is a $N$-vector of outcomes, and $\overline{y}$ is the sample mean of the
outcome.

Taking a QR decomposition of the design matrix, $\X = \Q\R$, where
$\Q^\top \Q = \mathbf{I}$ and $\R$ is upper triangular, we can write the
ordinary least squares (OLS) solution for the regression coefficients as
$$\hatbeta = \left(\X^\top \X \right)^{-1} \X^\top \y = \R^{-1} \Q^\top \y.$$
%
The QR decomposition is often used for improved numerical stability (see the
familiar {\tt lm} function in R), but, as we outline below, it is also useful
for thinking about priors in a Bayesian version of the linear model.


\section{Specification of priors}
\label{sec:priors}

The key innovation in this paper is the prior for the parameters in the
QR-reparameterized model, which can be thought of as a prior on the correlations
between the outcome $\y$ and the columns of the orthogonal matrix $\Q$. To
understand this prior, we start with the equations that characterize the maximum
likelihood solutions \emph{before} observing the data $\left(\y, \X\right)$.

Let $\boldtheta = \Q^\top \y$. We can write the $k$-th element of the vector
$\boldtheta$ as
$$\theta_k = \rho_k \, \sigma_y \, \sqrt{N - 1},$$
where $\rho_k$ is the correlation between the $k$th column of $\Q$ and the
outcome, $\sigma_y$ is the marginal standard deviation of the outcome, and
$1/\sqrt{N-1}$ is the standard deviation of the $k$ column of $\Q$. Then let
$\boldrho = \sqrt{r^2}\u$, where $\u$ is a unit vector that is
uniformally distributed on the surface of a hypersphere. Consequently,
$\u^\top \u = 1$ implies that the sum of squared correlations is
$\boldrho^\top \boldrho = r^2$, the familiar coefficient of determination for
the linear model.

\subsection{Prior for \Rsq}
\label{subsec:r2prior}
An uninformative prior on \Rsq would be standard uniform, which is a special
case of a $\Betadist{a}{b}$ distribution with shape parameters $a = b = 1$.
A non-uniform prior on \Rsq is somewhat analogous to ridge
regression, which is popular in data mining and produces better out-of-sample
predictions than least squares because it penalizes $\boldbeta^\top \boldbeta$,
usually after standardizing the predictors. In our case, an informative prior on
\Rsq effectively penalizes $\boldrho^\top \boldrho$, which encourages the
regression coefficients $\boldbeta = \R^{-1} \boldtheta$ to be closer to the
origin.

Consider a correlation matrix among both the outcome and the predictors of our
reparameterized model. \citeA{lkj} derives a distribution for a correlation
matrix that depends on a single shape parameter $\eta > 0$ and implies that the
conditional variance of one variable given the remaining $K$ variables is
distributed $\Betadist{\eta}{\halfK}$. In our case, this means that the
conditional variance of $\y$ given the predictors is
$$(1 - r^2) \sim \Betadist{\eta}{\halfK}$$
and, from the reflection symmetry of the Beta distribution, it follows that
$$r^2 \sim \Betadist{\halfK}{\eta}.$$
Any available prior information about the location of \Rsq, which we will denote
$\locRsq$, can be used to choose a value of the hyperparameter $\eta$. The
following are four ways of implying the value of $\eta$ by taking
$\locRsq$ to be (1) the prior mode of $r^2$, (2) the prior median of $r^2$,
(3) the prior mean of $r^2$, and (4) the prior mean of $\log{r^2}$:

\begin{enumerate}
\item $\locRsq \in \left(0,1\right)$ is the prior mode, where the mode of a
$\Betadist{\halfK}{\eta}$ distribution is
$\left(\halfK - 1\right) / \left(\halfK + \eta - 2\right)$.
This is only valid for $K \geq 2$ as otherwise the mode does not exist. If
the mode does exist we obtain
$$\eta = \frac{\halfK \left(1 - \locRsq\right) + 2\locRsq - 1}{\locRsq}.$$

\item $\locRsq \in \left(0,1\right)$ is the prior mean, where the mean of a
$\Betadist{\halfK}{\eta}$ distribution is
$\left(\halfK\right) / \left(\halfK + \eta\right)$. In this case we have
$$\eta = \frac{\halfK \left(1 - \locRsq \right)}{\locRsq}.$$


\item $\locRsq \in \left(0,1\right)$ is the prior median. The median of a
$\Betadist{\halfK}{\eta}$ distribution is not available in closed form, but if
$K > 2$ the median is approximately equal to \newline
$\left(\halfK - \frac{1}{3}\right) / \left(\halfK + \eta - \frac{2}{3}\right)$
\cite{kerman}. However, even if $K \leq 2$, we can numerically solve for the
value of $\eta$ that is consistent with a given prior median.

\item $\locRsq \in \left(-\infty,0\right)$ is the prior expectation of
$\log{r^2}$, which can be expressed in terms of the Digamma function
$\Digamma{\cdot}$ as
$E\left(\log{r^2}\right) = \Digamma{\halfK} - \Digamma{\halfK + \eta}$. Again,
given a prior value for the left-hand side we can numerically solve for the
corresponding value of the shape hyperparameter $\eta$.
\end{enumerate}

Each of these specifications of $\locRsq$ implies a value of the shape parameter
$\eta$, which is the single hyperparameter of the joint prior on the
coefficients. Smaller values for $\locRsq$ will correspond to larger values of
$\eta$, smaller prior correlations among the outcome and predictors, and a prior
density for the regression coefficients more concentrated around zero (see
Figure~\ref{fig:betaplot}).

\begin{figure}
\centering
\includegraphics[width=.67\textwidth]{betaplot.pdf}{\vspace{-.25cm}}
\caption{\em \small Implied marginal prior for one of the $K$ standardized
regression coefficients (computed from 100,000 draws). Here the value of $K$ is
fixed at $10$ and the plotted densities correspond to different values of the
hyperparameter $\eta$. For larger values of $\eta$ the prior is more
concentrated around zero.}
\label{fig:betaplot}
\end{figure}

\subsection{Log fit-ratio and prior for $\sigma_y$}
Let $\sigma_y = \omega s_y$, where $s_y$ is the sample standard deviation of the
outcome and $\omega > 0$ is an unknown scale parameter to be estimated. We use
the scale-invariant Jeffreys prior
$f_\omega \left(\omega\right) \propto 1 / \omega,$
which is proportional to a Jeffreys prior on the unknown $\sigma_y$,
$$f_{\sigma_y} \left(\sigma_y\right) \propto \frac{1}{\sigma_y}
= \frac{1}{\omega \widehat{\sigma}_y} \propto \frac{1}{\omega}.$$
This is the only prior that does not contravene Bayes' theorem in this
situation, as any other prior would result in the marginal standard deviation of
the outcome being a function of the estimated standard deviation of the outcome.
This parameterization and prior also makes it easy to work with any continuous
outcome variable, regardless of the unit of measurement.

When implementing the model we prefer to work with $\omega$ on the log scale
so we use the flat prior $f_\phi(\phi) \propto 1$, where $\phi =
\log{\omega}$, which is equivalent to the Jeffreys prior on $\omega$ itself. We
refer to $\phi$ as the \emph{log fit-ratio} since it is the logarithm of the
ratio of the marginal standard deviation of the outcome implied by the model to
the observed sample standard deviation,
%
$$\phi = \log{\omega} = \log{\frac{\sigma_y}{s_y}}.$$

We can interpret the log fit-ratio as a measure of underfitting or overfitting.
If $\phi = 0$, then the marginal standard deviation of the outcome is the same
as the sample standard deviation of the outcome. If $\phi > 0$, then the
marginal standard deviation of the outcome implied by the model exceeds the
sample standard deviation, which is to say that the model overfits the data.
Otherwise $\phi < 0$, in which case the marginal standard deviation of the
outcome implied by the model is less than the sample standard deviation and so
the model underfits the data (or the data-generating process is nonlinear).
Given the regularizing nature of the prior on \Rsq, a minor underfit would be
considered ideal if the goal is to obtain good out-of-sample predictions. If the
model badly underfits or overfits the data, then the model should be
reconsidered.

\subsection{Prior for $\alpha$}
We need not directly specify a prior for $\sigmaEps$ because our prior beliefs
about $\sigmaEps$ are already implied by our beliefs about $\omega$ and \Rsq via
the relation
$$\sigmaEps = \omega s_y \sqrt{1 - r^2}.$$
Thus, the only remaining distribution to specify is a prior for
$\alpha = \overline{y} - \overline{\mathbf{x}}^\top \R^{-1} \boldtheta$.
As a default, an improper uniform prior $f_\alpha(\alpha) \propto 1$ is possible
as the posterior will still be proper.

\section{Posterior}
\label{sec:posterior}

The previous sections imply a joint posterior distribution for the primitive
parameters $\left(\phi, \alpha, \u, r^2 \right)$. As demonstrated in
Section~\ref{sec:example}, draws from the posterior can be obtained via Markov
chain Monte Carlo (MCMC). We can then easily recover the parameters of primary
interest from the primitive parameters as
%
\begin{align*}
\sigma_y &= \omega s_y = e^\phi s_y \\
\sigmaEps &= \sigma_y \sqrt{1 - r^2} \\
\boldbeta &= \R^{-1} \u \, \sigma_y \sqrt{r^2 \left(N-1\right)},
\end{align*}
%
where each of these computations is performed for each draw from the posterior.

\section{Example}
\label{sec:example}

\lstset{
    language=R,
    basicstyle=\footnotesize\ttfamily,
    literate={~} {$\sim$}{1}
    }

We have implemented the proposed prior distribution in the {\tt stan\_lm}
function in the {\tt rstanarm} R package \cite{rstanarm}.\footnote{Source
code can be found at \url{https://github.com/stan-dev/rstanarm}.}
Here we provide a brief demonstration.

We will utilize an example from the {\tt HSAUR3} R package, which is used in the
third edition of \emph{A Handbook of Statistical Analyses Using R}
\cite{HSAUR3-book}. The model in section 5.3.1 analyzes an experiment where
clouds were seeded with different amounts of silver iodide to see if there was
increased rainfall. This effect could vary according to covariates, which
(except for time) are interacted with the treatment variable. Most people would
probably be skeptical that cloud hacking could explain very much of the
variation in rainfall and thus the prior mode of the \Rsq should be fairly
small.

The frequentist estimator of this model can be replicated in R by executing

\vspace{.5cm}
\begin{lstlisting}[frame=lines]
> data("clouds", package = "HSAUR3")
> mod <- rainfall ~ seeding * (sne+cloudcover+prewetness+echomotion) + time
> ols <- lm(formula = mod, data = clouds)
\end{lstlisting}
\vspace{.5cm}

\noindent from which we obtain the following estimated coefficients:

\vspace{.5cm}
\begin{lstlisting}[frame=lines]
> s <- summary(ols)$coef
> round(s[, 1:2], 2)
                                  Estimate Std. Error
(Intercept)                        -0.35       2.79
seedingyes                         15.68       4.45
sne                                 0.42       0.84
cloudcover                          0.39       0.22
prewetness                          4.11       3.60
echomotionstationary                3.15       1.93
time                               -0.04       0.03
seedingyes:sne                     -3.20       1.27
seedingyes:cloudcover              -0.49       0.24
seedingyes:prewetness              -2.56       4.48
seedingyes:echomotionstationary    -0.56       2.64
\end{lstlisting}
\vspace{.5cm}

Note that we have \emph{not} looked at the estimated \Rsq or $\sigma$ for the
OLS model. We can estimate a Bayesian version of this model using the {\tt
rstanarm} package, which will use Stan \cite{stan} to draw from the posterior
distribution via MCMC. To fit the model we simply prepend {\tt stan\_} to the
{\tt lm} call and specify a prior mode for \Rsq using the {\tt R2} function.%
\footnote{The {\tt what} argument to the {\tt R2} function can take the values
{\tt mode}, {\tt mean}, {\tt median}, and {\tt log}, corresponding to the four
methods for choosing $\eta$ via the specification of $\locRsq$ detailed in
Section~\ref{subsec:r2prior}.}

\vspace{.5cm}
\begin{lstlisting}[frame=lines]
> library("rstanarm")
> post <- stan_lm(formula = mod, data = clouds,
                  prior = R2(location = 0.2, what = "mode"))
> print(post)
                                Median MAD_SD
(Intercept)                      2.4    2.2
seedingyes                       6.5    3.5
sne                              0.2    0.7
cloudcover                       0.2    0.2
prewetness                       1.8    2.8
echomotionstationary             1.3    1.5
time                             0.0    0.0
seedingyes:sne                  -1.3    1.0
seedingyes:cloudcover           -0.2    0.2
seedingyes:prewetness           -1.1    3.4
seedingyes:echomotionstationary -0.2    2.0
sigma                            2.6    0.4
log-fit_ratio                    0.0    0.1
R2                               0.3    0.1
\end{lstlisting}
\vspace{.5cm}

The point estimates from the Bayesian model, which are represented by the
posterior medians, appear quite different from the OLS estimates. However, the
log fit-ratio ($\log{\omega}$) is estimated to be about $0$, which indicates a
good fit of the model to the data. It would be safe to conclude that the OLS
estimator considerably overfits the data since there are only $24$ observations
with which to estimate $12$ parameters and no prior information is leveraged. In
general, we would expect the prior derived in this paper to be well-suited in
situations like the one above, where there are many predictors relative to the
number of observations.


\section{Conclusion}

Priors can be easy or hard for applied researchers to \emph{specify} and easy or
hard for applied researchers to \emph{conceptualize}. Traditional shortcut
priors on regression coefficients are often used because they are both easy to
specify and to conceptualize. The informative prior on \Rsq proposed in this
paper is more difficult to conceptualize, but with the implementation of the
{\tt rstanarm} R package it is now equally easy to specify.


\nocite{guan}
\nocite{Rcore}
\nocite{HSAUR3-package}
\bibliography{references.bib}


% End document
\end{document}
